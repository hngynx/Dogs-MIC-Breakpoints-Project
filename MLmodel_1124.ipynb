{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow is already installed. Version: 2.18.0\n",
      "Hyperopt is already installed. Version: 0.2.7\n",
      "LightGBM Model Accuracy: 0.7222\n",
      "LightGBM Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.96      0.84        80\n",
      "           1       0.25      0.04      0.06        28\n",
      "\n",
      "    accuracy                           0.72       108\n",
      "   macro avg       0.50      0.50      0.45       108\n",
      "weighted avg       0.61      0.72      0.64       108\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "\n",
    "# Function to install a package\n",
    "def install_package(package_name):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package_name])\n",
    "\n",
    "# Install TensorFlow\n",
    "try:\n",
    "    import tensorflow\n",
    "    print(f\"TensorFlow is already installed. Version: {tensorflow.__version__}\")\n",
    "except ModuleNotFoundError:\n",
    "    print(\"TensorFlow not found. Installing TensorFlow...\")\n",
    "    install_package(\"tensorflow\")\n",
    "    print(\"TensorFlow installation complete.\")\n",
    "\n",
    "\n",
    "# Install hyperopt\n",
    "try:\n",
    "    import hyperopt\n",
    "    print(f\"Hyperopt is already installed. Version: {hyperopt.__version__}\")\n",
    "except ModuleNotFoundError:\n",
    "    print(\"Hyperopt not found. Installing Hyperopt...\")\n",
    "    install_package(\"hyperopt\")\n",
    "    print(\"Hyperopt installation complete.\")\n",
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization Libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine Learning Libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Gradient Boosting Libraries\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set seaborn style\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = r'C:\\Users\\Hubert N\\Downloads\\8.31.22 STAR Data Spreadsheet (2).csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Define columns of interest\n",
    "columns_of_interest = [\n",
    "    \"Sex (0 = female; 1 = male)\",\n",
    "    \"Neuter status (0 = castrated; 1 = not castrated)\",\n",
    "    \"Age\",\n",
    "    \"Duration of U-cath placement (days)\",\n",
    "    \"U-cath placement (no = 0; yes = 1)\",\n",
    "    \"Number of vet visits\",\n",
    "    'Collection Type (Cysto = 0; other =1; unknown = 2)',\n",
    "    'Color (0 = yellow/dark yellow/amber; 1 = straw; 2 = red/pink; 3 = brown/light & dark brown; 4 = other)',\n",
    "    'Clarity (0 = clear; 1 = opaque/cloudy; 2 = SLhazy; 3 = hazy)',\n",
    "    'USG', 'PH', 'Protein', 'Glucose', 'Ketones',\n",
    "    'Bilirubin', 'Hemoprotein', 'Sediment Vol', 'Volume',\n",
    "    'Lipid Layer', 'WBC (simplified)', 'Pyuria (1 if >/=4)',\n",
    "    'RBC (simplified)', 'Crystals (0 = none 1 = rare; 2 = few; 3 = mod; 4 = many)',\n",
    "    'Triple Phos', 'Bilirubin.1',\n",
    "    'Epithelial Cells-Transitional', 'Epithelial Cells-Squamous',\n",
    "    'Epithelial Cells-Renal', 'Epithelial Cells-Caudate',\n",
    "    'Casts-Hyaline', 'Casts-Granular',\n",
    "    'Bacteria quantity', 'Bac Type', 'Lipid Droplets', 'Sperm'\n",
    "]\n",
    "\n",
    "# Ensure all columns are present\n",
    "missing_columns = [col for col in columns_of_interest if col not in df.columns]\n",
    "if missing_columns:\n",
    "    print(\"Missing columns:\", missing_columns)\n",
    "else:\n",
    "    features = df[columns_of_interest]\n",
    "\n",
    "    # Filter and encode the target variable\n",
    "    df = df[df[\"Ampicillin\"].isin([\"R\", \"S\"])]  # Keep only 'R' and 'S' labels\n",
    "    df['Ampicillin'] = df['Ampicillin'].map({'S': 0, 'R': 1})\n",
    "    target = df['Ampicillin']\n",
    "\n",
    "    # Ensure features and target are aligned\n",
    "    features = features.loc[target.index]\n",
    "\n",
    "    # Manually define categorical columns\n",
    "    categorical_cols = [\n",
    "        \"Sex (0 = female; 1 = male)\",\n",
    "        \"Neuter status (0 = castrated; 1 = not castrated)\",\n",
    "        \"U-cath placement (no = 0; yes = 1)\",\n",
    "        'Collection Type (Cysto = 0; other =1; unknown = 2)',\n",
    "        'Color (0 = yellow/dark yellow/amber; 1 = straw; 2 = red/pink; 3 = brown/light & dark brown; 4 = other)',\n",
    "        'Clarity (0 = clear; 1 = opaque/cloudy; 2 = SLhazy; 3 = hazy)',\n",
    "        'Lipid Layer', 'Pyuria (1 if >/=4)',\n",
    "        'Crystals (0 = none 1 = rare; 2 = few; 3 = mod; 4 = many)',\n",
    "        'Triple Phos', 'Bilirubin.1',\n",
    "        'Epithelial Cells-Transitional', 'Epithelial Cells-Squamous',\n",
    "        'Epithelial Cells-Renal', 'Epithelial Cells-Caudate',\n",
    "        'Casts-Hyaline', 'Casts-Granular',\n",
    "        'Bacteria quantity', 'Bac Type', 'Lipid Droplets', 'Sperm'\n",
    "    ]\n",
    "\n",
    "    # Manually define numerical columns\n",
    "    numerical_cols = [col for col in features.columns if col not in categorical_cols]\n",
    "\n",
    "    # Convert categorical columns to 'object' dtype\n",
    "    features[categorical_cols] = features[categorical_cols].astype('object')\n",
    "\n",
    "    # Impute missing values for numerical columns with median\n",
    "    numerical_imputer = SimpleImputer(strategy='median')\n",
    "    features[numerical_cols] = numerical_imputer.fit_transform(features[numerical_cols])\n",
    "\n",
    "    # Impute missing values for categorical columns with mode\n",
    "    categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
    "    features[categorical_cols] = categorical_imputer.fit_transform(features[categorical_cols])\n",
    "\n",
    "    # One-hot encode categorical variables\n",
    "    features = pd.get_dummies(features, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "    # Standardize numerical features\n",
    "    scaler = StandardScaler()\n",
    "    features[numerical_cols] = scaler.fit_transform(features[numerical_cols])\n",
    "\n",
    "    # Split into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        features, target, test_size=0.2, random_state=42, stratify=target\n",
    "    )\n",
    "\n",
    "    # -------------------- LightGBM with LGBMClassifier --------------------\n",
    "\n",
    "    # Split the dataset into training and validation sets\n",
    "    X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
    "        X_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    "    )\n",
    "\n",
    "    # Define parameters\n",
    "    lgb_params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'binary_logloss',\n",
    "        'verbosity': -1,\n",
    "        'boosting_type': 'gbdt',\n",
    "        'num_leaves': 31,\n",
    "        'learning_rate': 0.05,\n",
    "        'feature_fraction': 0.9\n",
    "    }\n",
    "\n",
    "    # Initialize LGBMClassifier\n",
    "    lgb_clf = LGBMClassifier(**lgb_params, n_estimators=100)\n",
    "\n",
    "    # Fit the model\n",
    "    lgb_clf.fit(\n",
    "        X_train_split, y_train_split,\n",
    "        eval_set=[(X_val_split, y_val_split)],\n",
    "        eval_metric='logloss'\n",
    "    )\n",
    "\n",
    "    # Predict on the test set\n",
    "    y_pred_lgb = lgb_clf.predict(X_test)\n",
    "\n",
    "    # Evaluate the model\n",
    "    accuracy_lgb = accuracy_score(y_test, y_pred_lgb)\n",
    "    print(f\"LightGBM Model Accuracy: {accuracy_lgb:.4f}\")\n",
    "\n",
    "    print(\"LightGBM Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred_lgb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-24 04:31:32,424] A new study created in memory with name: no-name-e59ed1db-9d8c-4518-bc14-bcb95ab992d3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best oversampling method: SMOTE with AUC: 0.9468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-24 04:31:33,826] Trial 0 finished with value: 0.9054687499999999 and parameters: {'max_depth': 4, 'num_leaves': 24, 'learning_rate': 0.009897060728725215, 'n_estimators': 611, 'min_child_samples': 13, 'subsample': 0.716047607256282, 'colsample_bytree': 0.5480697263274124, 'reg_alpha': 3.151981407188883, 'reg_lambda': 0.0021012713122172157}. Best is trial 0 with value: 0.9054687499999999.\n",
      "[I 2024-11-24 04:31:34,310] Trial 1 finished with value: 0.89171875 and parameters: {'max_depth': 5, 'num_leaves': 40, 'learning_rate': 0.014485704892427365, 'n_estimators': 739, 'min_child_samples': 79, 'subsample': 0.6752690562404828, 'colsample_bytree': 0.6877012762729982, 'reg_alpha': 0.00367966087309512, 'reg_lambda': 9.385358133702345e-05}. Best is trial 0 with value: 0.9054687499999999.\n",
      "[I 2024-11-24 04:31:34,753] Trial 2 finished with value: 0.8729687499999998 and parameters: {'max_depth': 15, 'num_leaves': 21, 'learning_rate': 0.005815149937267976, 'n_estimators': 640, 'min_child_samples': 91, 'subsample': 0.7156329929418338, 'colsample_bytree': 0.9617630042235874, 'reg_alpha': 0.029021508842494607, 'reg_lambda': 0.0008085579152313072}. Best is trial 0 with value: 0.9054687499999999.\n",
      "[I 2024-11-24 04:31:35,157] Trial 3 finished with value: 0.9109375 and parameters: {'max_depth': 14, 'num_leaves': 35, 'learning_rate': 0.07354866939061608, 'n_estimators': 503, 'min_child_samples': 16, 'subsample': 0.728773765235865, 'colsample_bytree': 0.7886921282886903, 'reg_alpha': 1.4292956437097646, 'reg_lambda': 0.09346347178111396}. Best is trial 3 with value: 0.9109375.\n",
      "[I 2024-11-24 04:31:35,477] Trial 4 finished with value: 0.86609375 and parameters: {'max_depth': 4, 'num_leaves': 137, 'learning_rate': 0.005330620345083274, 'n_estimators': 476, 'min_child_samples': 96, 'subsample': 0.5887021315823796, 'colsample_bytree': 0.7867583905258178, 'reg_alpha': 1.4017027147554801, 'reg_lambda': 0.00016998786937470387}. Best is trial 3 with value: 0.9109375.\n",
      "[I 2024-11-24 04:31:37,059] Trial 5 finished with value: 0.9076562499999999 and parameters: {'max_depth': 6, 'num_leaves': 129, 'learning_rate': 0.009970971182639001, 'n_estimators': 763, 'min_child_samples': 32, 'subsample': 0.8265536058125602, 'colsample_bytree': 0.5416888645253177, 'reg_alpha': 1.5500052719955707e-05, 'reg_lambda': 1.1430172668735039e-05}. Best is trial 3 with value: 0.9109375.\n",
      "[I 2024-11-24 04:31:37,467] Trial 6 finished with value: 0.8934375000000001 and parameters: {'max_depth': 5, 'num_leaves': 109, 'learning_rate': 0.016728283021731984, 'n_estimators': 353, 'min_child_samples': 50, 'subsample': 0.5066082182769972, 'colsample_bytree': 0.9137268266686618, 'reg_alpha': 2.2861738076056466e-05, 'reg_lambda': 1.8602195848847622}. Best is trial 3 with value: 0.9109375.\n",
      "[I 2024-11-24 04:31:37,919] Trial 7 finished with value: 0.90296875 and parameters: {'max_depth': 15, 'num_leaves': 33, 'learning_rate': 0.05240885243498288, 'n_estimators': 262, 'min_child_samples': 38, 'subsample': 0.869392928250926, 'colsample_bytree': 0.6116152982865002, 'reg_alpha': 3.958697613018282e-05, 'reg_lambda': 0.11337557074966491}. Best is trial 3 with value: 0.9109375.\n",
      "[I 2024-11-24 04:31:38,568] Trial 8 finished with value: 0.88234375 and parameters: {'max_depth': 13, 'num_leaves': 121, 'learning_rate': 0.03106245717706467, 'n_estimators': 729, 'min_child_samples': 68, 'subsample': 0.5024719394254635, 'colsample_bytree': 0.9054290195165889, 'reg_alpha': 0.00016652813420094166, 'reg_lambda': 0.0005398289092874067}. Best is trial 3 with value: 0.9109375.\n",
      "[I 2024-11-24 04:31:38,926] Trial 9 finished with value: 0.8943749999999999 and parameters: {'max_depth': 3, 'num_leaves': 33, 'learning_rate': 0.015511172450574087, 'n_estimators': 442, 'min_child_samples': 44, 'subsample': 0.7635247588810143, 'colsample_bytree': 0.7505276012267179, 'reg_alpha': 0.001942609903247363, 'reg_lambda': 8.726070206671581}. Best is trial 3 with value: 0.9109375.\n",
      "[I 2024-11-24 04:31:39,498] Trial 10 finished with value: 0.9034374999999999 and parameters: {'max_depth': 11, 'num_leaves': 70, 'learning_rate': 0.18450872206439325, 'n_estimators': 949, 'min_child_samples': 5, 'subsample': 0.9790624455080519, 'colsample_bytree': 0.8343889099330749, 'reg_alpha': 0.14360291366393574, 'reg_lambda': 0.058774400082502565}. Best is trial 3 with value: 0.9109375.\n",
      "[I 2024-11-24 04:31:39,851] Trial 11 finished with value: 0.87625 and parameters: {'max_depth': 8, 'num_leaves': 78, 'learning_rate': 0.0902715565374602, 'n_estimators': 975, 'min_child_samples': 27, 'subsample': 0.8459377666758281, 'colsample_bytree': 0.6567780818515404, 'reg_alpha': 9.88979222600757, 'reg_lambda': 0.021965365390269286}. Best is trial 3 with value: 0.9109375.\n",
      "[I 2024-11-24 04:31:40,270] Trial 12 finished with value: 0.9032812499999998 and parameters: {'max_depth': 8, 'num_leaves': 95, 'learning_rate': 0.0361696076383343, 'n_estimators': 128, 'min_child_samples': 25, 'subsample': 0.8259329487846659, 'colsample_bytree': 0.5014108554617309, 'reg_alpha': 0.2700387167160328, 'reg_lambda': 1.0035353390612386e-05}. Best is trial 3 with value: 0.9109375.\n",
      "[I 2024-11-24 04:31:41,408] Trial 13 finished with value: 0.91421875 and parameters: {'max_depth': 10, 'num_leaves': 55, 'learning_rate': 0.08106400097544991, 'n_estimators': 812, 'min_child_samples': 28, 'subsample': 0.9235597133736317, 'colsample_bytree': 0.8329332390065408, 'reg_alpha': 0.000673696767075276, 'reg_lambda': 0.3678556272659686}. Best is trial 13 with value: 0.91421875.\n",
      "[I 2024-11-24 04:31:42,861] Trial 14 finished with value: 0.916875 and parameters: {'max_depth': 11, 'num_leaves': 56, 'learning_rate': 0.10868194565964863, 'n_estimators': 870, 'min_child_samples': 16, 'subsample': 0.9511825161731655, 'colsample_bytree': 0.837411701168094, 'reg_alpha': 0.0007870814349347866, 'reg_lambda': 0.5794584397460019}. Best is trial 14 with value: 0.916875.\n",
      "[I 2024-11-24 04:31:43,554] Trial 15 finished with value: 0.8896874999999999 and parameters: {'max_depth': 11, 'num_leaves': 55, 'learning_rate': 0.16118077389478905, 'n_estimators': 864, 'min_child_samples': 60, 'subsample': 0.9988569244101173, 'colsample_bytree': 0.8310611133045487, 'reg_alpha': 0.0004163457432423112, 'reg_lambda': 0.7563666217568198}. Best is trial 14 with value: 0.916875.\n",
      "[I 2024-11-24 04:31:44,877] Trial 16 finished with value: 0.9154687499999999 and parameters: {'max_depth': 11, 'num_leaves': 60, 'learning_rate': 0.11497186084028697, 'n_estimators': 845, 'min_child_samples': 20, 'subsample': 0.9233919513143547, 'colsample_bytree': 0.8623316986228965, 'reg_alpha': 0.0004773706806694803, 'reg_lambda': 0.4726134731623701}. Best is trial 14 with value: 0.916875.\n",
      "[I 2024-11-24 04:31:47,718] Trial 17 finished with value: 0.9234375 and parameters: {'max_depth': 12, 'num_leaves': 68, 'learning_rate': 0.11584793969505817, 'n_estimators': 918, 'min_child_samples': 6, 'subsample': 0.9300909113968389, 'colsample_bytree': 0.9918996248995844, 'reg_alpha': 0.009077419867561115, 'reg_lambda': 8.089619079988235}. Best is trial 17 with value: 0.9234375.\n",
      "[I 2024-11-24 04:31:51,012] Trial 18 finished with value: 0.9220312500000001 and parameters: {'max_depth': 13, 'num_leaves': 94, 'learning_rate': 0.13073945217985422, 'n_estimators': 988, 'min_child_samples': 5, 'subsample': 0.9202481555919237, 'colsample_bytree': 0.9864228889198039, 'reg_alpha': 0.02035510149531924, 'reg_lambda': 9.306548596729918}. Best is trial 17 with value: 0.9234375.\n",
      "[I 2024-11-24 04:31:53,904] Trial 19 finished with value: 0.91875 and parameters: {'max_depth': 13, 'num_leaves': 95, 'learning_rate': 0.04952411663398127, 'n_estimators': 981, 'min_child_samples': 7, 'subsample': 0.8941973867490394, 'colsample_bytree': 0.9714396225099047, 'reg_alpha': 0.018252559575181838, 'reg_lambda': 6.037200202234839}. Best is trial 17 with value: 0.9234375.\n",
      "[I 2024-11-24 04:31:55,251] Trial 20 finished with value: 0.91671875 and parameters: {'max_depth': 13, 'num_leaves': 91, 'learning_rate': 0.14303041741709946, 'n_estimators': 632, 'min_child_samples': 5, 'subsample': 0.7954080535144507, 'colsample_bytree': 0.9847901301348797, 'reg_alpha': 0.08047735415024015, 'reg_lambda': 3.5482686820817637}. Best is trial 17 with value: 0.9234375.\n",
      "[I 2024-11-24 04:31:58,449] Trial 21 finished with value: 0.9265625000000001 and parameters: {'max_depth': 13, 'num_leaves': 108, 'learning_rate': 0.05954766162318438, 'n_estimators': 987, 'min_child_samples': 6, 'subsample': 0.88229700490954, 'colsample_bytree': 0.9984667191225268, 'reg_alpha': 0.011414422650943876, 'reg_lambda': 7.105517483063494}. Best is trial 21 with value: 0.9265625000000001.\n",
      "[I 2024-11-24 04:32:00,320] Trial 22 finished with value: 0.9206249999999999 and parameters: {'max_depth': 12, 'num_leaves': 109, 'learning_rate': 0.06251382765682835, 'n_estimators': 910, 'min_child_samples': 13, 'subsample': 0.9002039413288494, 'colsample_bytree': 0.928237344871999, 'reg_alpha': 0.005101456569448351, 'reg_lambda': 1.8355698487932046}. Best is trial 21 with value: 0.9265625000000001.\n",
      "[I 2024-11-24 04:32:01,569] Trial 23 finished with value: 0.90515625 and parameters: {'max_depth': 9, 'num_leaves': 111, 'learning_rate': 0.035280985727386624, 'n_estimators': 999, 'min_child_samples': 36, 'subsample': 0.9627956560397277, 'colsample_bytree': 0.9921790390851477, 'reg_alpha': 0.032112231790696326, 'reg_lambda': 8.36014220027455}. Best is trial 21 with value: 0.9265625000000001.\n",
      "[I 2024-11-24 04:32:03,771] Trial 24 finished with value: 0.9228125 and parameters: {'max_depth': 14, 'num_leaves': 80, 'learning_rate': 0.1197507876278269, 'n_estimators': 906, 'min_child_samples': 9, 'subsample': 0.8615468358799155, 'colsample_bytree': 0.9392802491870947, 'reg_alpha': 0.00924950530658114, 'reg_lambda': 2.458636870405265}. Best is trial 21 with value: 0.9265625000000001.\n",
      "[I 2024-11-24 04:32:05,016] Trial 25 finished with value: 0.91265625 and parameters: {'max_depth': 14, 'num_leaves': 75, 'learning_rate': 0.09714341063259703, 'n_estimators': 796, 'min_child_samples': 22, 'subsample': 0.8634912848067234, 'colsample_bytree': 0.9408975915944346, 'reg_alpha': 0.0064148001271389105, 'reg_lambda': 1.4824025426146636}. Best is trial 21 with value: 0.9265625000000001.\n",
      "[I 2024-11-24 04:32:06,008] Trial 26 finished with value: 0.9237499999999998 and parameters: {'max_depth': 14, 'num_leaves': 85, 'learning_rate': 0.0501101105683147, 'n_estimators': 907, 'min_child_samples': 14, 'subsample': 0.7903518686919262, 'colsample_bytree': 0.8874117386592224, 'reg_alpha': 0.39741267259016666, 'reg_lambda': 0.009303198896722946}. Best is trial 21 with value: 0.9265625000000001.\n",
      "[I 2024-11-24 04:32:07,066] Trial 27 finished with value: 0.9123437499999999 and parameters: {'max_depth': 12, 'num_leaves': 146, 'learning_rate': 0.025804108125474665, 'n_estimators': 696, 'min_child_samples': 19, 'subsample': 0.7853132328068866, 'colsample_bytree': 0.8765715599992951, 'reg_alpha': 0.42405852489082774, 'reg_lambda': 0.005908146041119609}. Best is trial 21 with value: 0.9265625000000001.\n",
      "[I 2024-11-24 04:32:07,986] Trial 28 finished with value: 0.89640625 and parameters: {'max_depth': 15, 'num_leaves': 67, 'learning_rate': 0.044532620744598315, 'n_estimators': 915, 'min_child_samples': 44, 'subsample': 0.6720743115611731, 'colsample_bytree': 0.9071224480701608, 'reg_alpha': 0.0809257831496009, 'reg_lambda': 0.013059929890606677}. Best is trial 21 with value: 0.9265625000000001.\n",
      "[I 2024-11-24 04:32:09,216] Trial 29 finished with value: 0.9165624999999998 and parameters: {'max_depth': 12, 'num_leaves': 85, 'learning_rate': 0.02171810127853064, 'n_estimators': 569, 'min_child_samples': 12, 'subsample': 0.814394252434377, 'colsample_bytree': 0.8797180998933468, 'reg_alpha': 0.46196808614196433, 'reg_lambda': 0.0016897974256409108}. Best is trial 21 with value: 0.9265625000000001.\n",
      "[I 2024-11-24 04:32:10,001] Trial 30 finished with value: 0.8900000000000001 and parameters: {'max_depth': 10, 'num_leaves': 104, 'learning_rate': 0.06537308916621591, 'n_estimators': 683, 'min_child_samples': 61, 'subsample': 0.8912744607620918, 'colsample_bytree': 0.9541181825650028, 'reg_alpha': 0.0018124370742155494, 'reg_lambda': 0.0051059399491581395}. Best is trial 21 with value: 0.9265625000000001.\n",
      "[I 2024-11-24 04:32:12,396] Trial 31 finished with value: 0.9243749999999998 and parameters: {'max_depth': 14, 'num_leaves': 80, 'learning_rate': 0.18578538650486134, 'n_estimators': 907, 'min_child_samples': 10, 'subsample': 0.8544104555552365, 'colsample_bytree': 0.9990256511735216, 'reg_alpha': 0.011620716059783166, 'reg_lambda': 3.126747050811338}. Best is trial 21 with value: 0.9265625000000001.\n",
      "[I 2024-11-24 04:32:13,541] Trial 32 finished with value: 0.9160937499999999 and parameters: {'max_depth': 14, 'num_leaves': 85, 'learning_rate': 0.1980888946999617, 'n_estimators': 809, 'min_child_samples': 13, 'subsample': 0.7584548597208014, 'colsample_bytree': 0.9611348070037609, 'reg_alpha': 0.003185214749184747, 'reg_lambda': 0.17555760617125837}. Best is trial 21 with value: 0.9265625000000001.\n",
      "[I 2024-11-24 04:32:14,133] Trial 33 finished with value: 0.91171875 and parameters: {'max_depth': 15, 'num_leaves': 66, 'learning_rate': 0.15670838959076763, 'n_estimators': 920, 'min_child_samples': 20, 'subsample': 0.9443938739423128, 'colsample_bytree': 0.989075306437687, 'reg_alpha': 0.05032942383722075, 'reg_lambda': 0.03176092585305081}. Best is trial 21 with value: 0.9265625000000001.\n",
      "[I 2024-11-24 04:32:15,888] Trial 34 finished with value: 0.9232812499999999 and parameters: {'max_depth': 14, 'num_leaves': 119, 'learning_rate': 0.06508056334535424, 'n_estimators': 879, 'min_child_samples': 11, 'subsample': 0.7197090755518806, 'colsample_bytree': 0.9995673784258061, 'reg_alpha': 0.014742649137452027, 'reg_lambda': 0.22786034659580334}. Best is trial 21 with value: 0.9265625000000001.\n",
      "[I 2024-11-24 04:32:16,446] Trial 35 finished with value: 0.91640625 and parameters: {'max_depth': 12, 'num_leaves': 44, 'learning_rate': 0.08351123756077083, 'n_estimators': 754, 'min_child_samples': 15, 'subsample': 0.6741173210084987, 'colsample_bytree': 0.8948327672842032, 'reg_alpha': 1.3908195719403815, 'reg_lambda': 3.556438962343045}. Best is trial 21 with value: 0.9265625000000001.\n",
      "[I 2024-11-24 04:32:17,142] Trial 36 finished with value: 0.8857812500000001 and parameters: {'max_depth': 13, 'num_leaves': 102, 'learning_rate': 0.04351079174267864, 'n_estimators': 839, 'min_child_samples': 82, 'subsample': 0.6391804090746147, 'colsample_bytree': 0.9478078637859271, 'reg_alpha': 0.00012842153040124076, 'reg_lambda': 0.9819778871851224}. Best is trial 21 with value: 0.9265625000000001.\n",
      "[I 2024-11-24 04:32:17,736] Trial 37 finished with value: 0.8999999999999999 and parameters: {'max_depth': 15, 'num_leaves': 86, 'learning_rate': 0.09939575015091123, 'n_estimators': 944, 'min_child_samples': 32, 'subsample': 0.803729594981737, 'colsample_bytree': 0.6982399187084289, 'reg_alpha': 0.16511174044514237, 'reg_lambda': 6.42298114221582e-05}. Best is trial 21 with value: 0.9265625000000001.\n",
      "[I 2024-11-24 04:32:18,664] Trial 38 finished with value: 0.92078125 and parameters: {'max_depth': 14, 'num_leaves': 47, 'learning_rate': 0.0613027174263128, 'n_estimators': 786, 'min_child_samples': 10, 'subsample': 0.834503495356606, 'colsample_bytree': 0.7904202703225836, 'reg_alpha': 0.769706295577852, 'reg_lambda': 3.9730990653679394}. Best is trial 21 with value: 0.9265625000000001.\n",
      "[I 2024-11-24 04:32:19,618] Trial 39 finished with value: 0.88828125 and parameters: {'max_depth': 10, 'num_leaves': 74, 'learning_rate': 0.007843981107907808, 'n_estimators': 683, 'min_child_samples': 25, 'subsample': 0.8801693358088313, 'colsample_bytree': 0.9243811447607307, 'reg_alpha': 4.8891863497456445, 'reg_lambda': 0.0010976994384755694}. Best is trial 21 with value: 0.9265625000000001.\n",
      "[I 2024-11-24 04:32:20,281] Trial 40 finished with value: 0.9062500000000001 and parameters: {'max_depth': 13, 'num_leaves': 127, 'learning_rate': 0.16676289943346145, 'n_estimators': 573, 'min_child_samples': 18, 'subsample': 0.7789925711921176, 'colsample_bytree': 0.9665360258607695, 'reg_alpha': 0.003015234921134853, 'reg_lambda': 0.0003393760988882447}. Best is trial 21 with value: 0.9265625000000001.\n",
      "[I 2024-11-24 04:32:21,722] Trial 41 finished with value: 0.9176562500000001 and parameters: {'max_depth': 14, 'num_leaves': 116, 'learning_rate': 0.06681007691806412, 'n_estimators': 881, 'min_child_samples': 11, 'subsample': 0.7243675302152699, 'colsample_bytree': 0.9977733816608697, 'reg_alpha': 0.025245712547805534, 'reg_lambda': 0.16058362714233312}. Best is trial 21 with value: 0.9265625000000001.\n",
      "[I 2024-11-24 04:32:23,252] Trial 42 finished with value: 0.9265625000000001 and parameters: {'max_depth': 15, 'num_leaves': 132, 'learning_rate': 0.05506854357979652, 'n_estimators': 952, 'min_child_samples': 9, 'subsample': 0.7091992634766107, 'colsample_bytree': 0.9980475689511323, 'reg_alpha': 0.01136508131892845, 'reg_lambda': 0.003873557912348564}. Best is trial 21 with value: 0.9265625000000001.\n",
      "[I 2024-11-24 04:32:24,927] Trial 43 finished with value: 0.91359375 and parameters: {'max_depth': 15, 'num_leaves': 137, 'learning_rate': 0.04275422578978976, 'n_estimators': 956, 'min_child_samples': 15, 'subsample': 0.6094555071826884, 'colsample_bytree': 0.9690807628541174, 'reg_alpha': 0.009647562741769962, 'reg_lambda': 0.00466219556346119}. Best is trial 21 with value: 0.9265625000000001.\n",
      "[I 2024-11-24 04:32:26,206] Trial 44 finished with value: 0.92140625 and parameters: {'max_depth': 15, 'num_leaves': 147, 'learning_rate': 0.055524580670757544, 'n_estimators': 933, 'min_child_samples': 5, 'subsample': 0.7458340750135092, 'colsample_bytree': 0.9284886950665434, 'reg_alpha': 0.05817306531441105, 'reg_lambda': 0.05238743950783961}. Best is trial 21 with value: 0.9265625000000001.\n",
      "[I 2024-11-24 04:32:27,267] Trial 45 finished with value: 0.911875 and parameters: {'max_depth': 14, 'num_leaves': 101, 'learning_rate': 0.07977958145255697, 'n_estimators': 997, 'min_child_samples': 23, 'subsample': 0.6973577699092423, 'colsample_bytree': 0.9520458766503499, 'reg_alpha': 0.0013517982255061617, 'reg_lambda': 0.0020232211137641014}. Best is trial 21 with value: 0.9265625000000001.\n",
      "[I 2024-11-24 04:32:28,349] Trial 46 finished with value: 0.9121874999999999 and parameters: {'max_depth': 12, 'num_leaves': 136, 'learning_rate': 0.028312353378771967, 'n_estimators': 482, 'min_child_samples': 30, 'subsample': 0.8601482179130827, 'colsample_bytree': 0.9010229752327346, 'reg_alpha': 0.006272569858669147, 'reg_lambda': 0.008494140780128267}. Best is trial 21 with value: 0.9265625000000001.\n",
      "[I 2024-11-24 04:32:29,476] Trial 47 finished with value: 0.9218750000000001 and parameters: {'max_depth': 7, 'num_leaves': 129, 'learning_rate': 0.03632526270113967, 'n_estimators': 372, 'min_child_samples': 9, 'subsample': 0.8434141825728538, 'colsample_bytree': 0.7518880511020897, 'reg_alpha': 0.1606410034767591, 'reg_lambda': 0.018410052018729865}. Best is trial 21 with value: 0.9265625000000001.\n",
      "[I 2024-11-24 04:32:30,637] Trial 48 finished with value: 0.8864062499999998 and parameters: {'max_depth': 13, 'num_leaves': 62, 'learning_rate': 0.019422180992099775, 'n_estimators': 836, 'min_child_samples': 74, 'subsample': 0.6955117745836166, 'colsample_bytree': 0.5812551921741829, 'reg_alpha': 0.04465405612333986, 'reg_lambda': 0.003185248614348581}. Best is trial 21 with value: 0.9265625000000001.\n",
      "[I 2024-11-24 04:32:31,358] Trial 49 finished with value: 0.9115624999999999 and parameters: {'max_depth': 15, 'num_leaves': 81, 'learning_rate': 0.13556205547531766, 'n_estimators': 883, 'min_child_samples': 18, 'subsample': 0.5500261038604288, 'colsample_bytree': 0.8546837061506457, 'reg_alpha': 0.011583729218407527, 'reg_lambda': 9.93345318530159e-05}. Best is trial 21 with value: 0.9265625000000001.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM AUC: 0.9266\n",
      "XGBoost AUC: 0.9031\n",
      "RandomForest AUC: 0.9468\n",
      "CatBoost AUC: 0.9261\n",
      "LogisticRegression AUC: 0.8575\n"
     ]
    },
    {
     "ename": "BrokenProcessPool",
     "evalue": "A task has failed to un-serialize. Please ensure that the arguments of the function are all picklable.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"c:\\Users\\Hubert N\\anaconda3\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 391, in _process_worker\n    call_item = call_queue.get(block=True, timeout=timeout)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Hubert N\\anaconda3\\Lib\\multiprocessing\\queues.py\", line 122, in get\n    return _ForkingPickler.loads(res)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\nAttributeError: Can't get attribute '_ThresholdScorer' on <module 'sklearn.metrics._scorer' from 'c:\\\\Users\\\\Hubert N\\\\anaconda3\\\\Lib\\\\site-packages\\\\sklearn\\\\metrics\\\\_scorer.py'>\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mBrokenProcessPool\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 272\u001b[0m\n\u001b[0;32m    270\u001b[0m \u001b[38;5;66;03m# Cross-Validation and Evaluation\u001b[39;00m\n\u001b[0;32m    271\u001b[0m cv \u001b[38;5;241m=\u001b[39m StratifiedKFold(n_splits\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m--> 272\u001b[0m cv_auc_scores \u001b[38;5;241m=\u001b[39m cross_val_score(best_lgb, X_resampled, y_resampled, cv\u001b[38;5;241m=\u001b[39mcv, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mroc_auc\u001b[39m\u001b[38;5;124m'\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCross-Validated AUC Scores for LightGBM: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcv_auc_scores\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    274\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMean Cross-Validated AUC Score for LightGBM: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcv_auc_scores\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Hubert N\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:515\u001b[0m, in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[0;32m    513\u001b[0m num_failed_fits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(fit_errors)\n\u001b[0;32m    514\u001b[0m num_fits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(results)\n\u001b[1;32m--> 515\u001b[0m fit_errors_counter \u001b[38;5;241m=\u001b[39m Counter(fit_errors)\n\u001b[0;32m    516\u001b[0m delimiter \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m80\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    517\u001b[0m fit_errors_summary \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[0;32m    518\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdelimiter\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed with the following error:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00merror\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    519\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m error, n \u001b[38;5;129;01min\u001b[39;00m fit_errors_counter\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m    520\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Hubert N\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:266\u001b[0m, in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[0;32m     96\u001b[0m     {\n\u001b[0;32m     97\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mestimator\u001b[39m\u001b[38;5;124m\"\u001b[39m: [HasMethods(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m)],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    138\u001b[0m     error_score\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mnan,\n\u001b[0;32m    139\u001b[0m ):\n\u001b[0;32m    140\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Evaluate metric(s) by cross-validation and also record fit/score times.\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \n\u001b[0;32m    142\u001b[0m \u001b[38;5;124;03m    Read more in the :ref:`User Guide <multimetric_cross_validation>`.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \n\u001b[0;32m    144\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;124;03m    ----------\u001b[39;00m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;124;03m    estimator : estimator object implementing 'fit'\u001b[39;00m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;124;03m        The object to use to fit the data.\u001b[39;00m\n\u001b[0;32m    148\u001b[0m \n\u001b[0;32m    149\u001b[0m \u001b[38;5;124;03m    X : {array-like, sparse matrix} of shape (n_samples, n_features)\u001b[39;00m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;124;03m        The data to fit. Can be for example a list, or an array.\u001b[39;00m\n\u001b[0;32m    151\u001b[0m \n\u001b[0;32m    152\u001b[0m \u001b[38;5;124;03m    y : array-like of shape (n_samples,) or (n_samples, n_outputs), default=None\u001b[39;00m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;124;03m        The target variable to try to predict in the case of\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;124;03m        supervised learning.\u001b[39;00m\n\u001b[0;32m    155\u001b[0m \n\u001b[0;32m    156\u001b[0m \u001b[38;5;124;03m    groups : array-like of shape (n_samples,), default=None\u001b[39;00m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;124;03m        Group labels for the samples used while splitting the dataset into\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;124;03m        train/test set. Only used in conjunction with a \"Group\" :term:`cv`\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;124;03m        instance (e.g., :class:`GroupKFold`).\u001b[39;00m\n\u001b[0;32m    160\u001b[0m \n\u001b[0;32m    161\u001b[0m \u001b[38;5;124;03m        .. versionchanged:: 1.4\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;124;03m            ``groups`` can only be passed if metadata routing is not enabled\u001b[39;00m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;124;03m            via ``sklearn.set_config(enable_metadata_routing=True)``. When routing\u001b[39;00m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;124;03m            is enabled, pass ``groups`` alongside other metadata via the ``params``\u001b[39;00m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;124;03m            argument instead. E.g.:\u001b[39;00m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;124;03m            ``cross_validate(..., params={'groups': groups})``.\u001b[39;00m\n\u001b[0;32m    167\u001b[0m \n\u001b[0;32m    168\u001b[0m \u001b[38;5;124;03m    scoring : str, callable, list, tuple, or dict, default=None\u001b[39;00m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;124;03m        Strategy to evaluate the performance of the cross-validated model on\u001b[39;00m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;124;03m        the test set.\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \n\u001b[0;32m    172\u001b[0m \u001b[38;5;124;03m        If `scoring` represents a single score, one can use:\u001b[39;00m\n\u001b[0;32m    173\u001b[0m \n\u001b[0;32m    174\u001b[0m \u001b[38;5;124;03m        - a single string (see :ref:`scoring_parameter`);\u001b[39;00m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;124;03m        - a callable (see :ref:`scoring`) that returns a single value.\u001b[39;00m\n\u001b[0;32m    176\u001b[0m \n\u001b[0;32m    177\u001b[0m \u001b[38;5;124;03m        If `scoring` represents multiple scores, one can use:\u001b[39;00m\n\u001b[0;32m    178\u001b[0m \n\u001b[0;32m    179\u001b[0m \u001b[38;5;124;03m        - a list or tuple of unique strings;\u001b[39;00m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;124;03m        - a callable returning a dictionary where the keys are the metric\u001b[39;00m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;124;03m          names and the values are the metric scores;\u001b[39;00m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;124;03m        - a dictionary with metric names as keys and callables a values.\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \n\u001b[0;32m    184\u001b[0m \u001b[38;5;124;03m        See :ref:`multimetric_grid_search` for an example.\u001b[39;00m\n\u001b[0;32m    185\u001b[0m \n\u001b[0;32m    186\u001b[0m \u001b[38;5;124;03m    cv : int, cross-validation generator or an iterable, default=None\u001b[39;00m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;124;03m        Determines the cross-validation splitting strategy.\u001b[39;00m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;124;03m        Possible inputs for cv are:\u001b[39;00m\n\u001b[0;32m    189\u001b[0m \n\u001b[0;32m    190\u001b[0m \u001b[38;5;124;03m        - None, to use the default 5-fold cross validation,\u001b[39;00m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;124;03m        - int, to specify the number of folds in a `(Stratified)KFold`,\u001b[39;00m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;124;03m        - :term:`CV splitter`,\u001b[39;00m\n\u001b[0;32m    193\u001b[0m \u001b[38;5;124;03m        - An iterable yielding (train, test) splits as arrays of indices.\u001b[39;00m\n\u001b[0;32m    194\u001b[0m \n\u001b[0;32m    195\u001b[0m \u001b[38;5;124;03m        For int/None inputs, if the estimator is a classifier and ``y`` is\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;124;03m        either binary or multiclass, :class:`StratifiedKFold` is used. In all\u001b[39;00m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;124;03m        other cases, :class:`KFold` is used. These splitters are instantiated\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;124;03m        with `shuffle=False` so the splits will be the same across calls.\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \n\u001b[0;32m    200\u001b[0m \u001b[38;5;124;03m        Refer :ref:`User Guide <cross_validation>` for the various\u001b[39;00m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;124;03m        cross-validation strategies that can be used here.\u001b[39;00m\n\u001b[0;32m    202\u001b[0m \n\u001b[0;32m    203\u001b[0m \u001b[38;5;124;03m        .. versionchanged:: 0.22\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;124;03m            ``cv`` default value if None changed from 3-fold to 5-fold.\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \n\u001b[0;32m    206\u001b[0m \u001b[38;5;124;03m    n_jobs : int, default=None\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;124;03m        Number of jobs to run in parallel. Training the estimator and computing\u001b[39;00m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;124;03m        the score are parallelized over the cross-validation splits.\u001b[39;00m\n\u001b[0;32m    209\u001b[0m \u001b[38;5;124;03m        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\u001b[39;00m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;124;03m        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\u001b[39;00m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;124;03m        for more details.\u001b[39;00m\n\u001b[0;32m    212\u001b[0m \n\u001b[0;32m    213\u001b[0m \u001b[38;5;124;03m    verbose : int, default=0\u001b[39;00m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;124;03m        The verbosity level.\u001b[39;00m\n\u001b[0;32m    215\u001b[0m \n\u001b[0;32m    216\u001b[0m \u001b[38;5;124;03m    fit_params : dict, default=None\u001b[39;00m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;124;03m        Parameters to pass to the fit method of the estimator.\u001b[39;00m\n\u001b[0;32m    218\u001b[0m \n\u001b[0;32m    219\u001b[0m \u001b[38;5;124;03m        .. deprecated:: 1.4\u001b[39;00m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;124;03m            This parameter is deprecated and will be removed in version 1.6. Use\u001b[39;00m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;124;03m            ``params`` instead.\u001b[39;00m\n\u001b[0;32m    222\u001b[0m \n\u001b[0;32m    223\u001b[0m \u001b[38;5;124;03m    params : dict, default=None\u001b[39;00m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;124;03m        Parameters to pass to the underlying estimator's ``fit``, the scorer,\u001b[39;00m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;124;03m        and the CV splitter.\u001b[39;00m\n\u001b[0;32m    226\u001b[0m \n\u001b[0;32m    227\u001b[0m \u001b[38;5;124;03m        .. versionadded:: 1.4\u001b[39;00m\n\u001b[0;32m    228\u001b[0m \n\u001b[0;32m    229\u001b[0m \u001b[38;5;124;03m    pre_dispatch : int or str, default='2*n_jobs'\u001b[39;00m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;124;03m        Controls the number of jobs that get dispatched during parallel\u001b[39;00m\n\u001b[0;32m    231\u001b[0m \u001b[38;5;124;03m        execution. Reducing this number can be useful to avoid an\u001b[39;00m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;124;03m        explosion of memory consumption when more jobs get dispatched\u001b[39;00m\n\u001b[0;32m    233\u001b[0m \u001b[38;5;124;03m        than CPUs can process. This parameter can be:\u001b[39;00m\n\u001b[0;32m    234\u001b[0m \n\u001b[0;32m    235\u001b[0m \u001b[38;5;124;03m            - An int, giving the exact number of total jobs that are\u001b[39;00m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;124;03m              spawned\u001b[39;00m\n\u001b[0;32m    237\u001b[0m \n\u001b[0;32m    238\u001b[0m \u001b[38;5;124;03m            - A str, giving an expression as a function of n_jobs,\u001b[39;00m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;124;03m              as in '2*n_jobs'\u001b[39;00m\n\u001b[0;32m    240\u001b[0m \n\u001b[0;32m    241\u001b[0m \u001b[38;5;124;03m    return_train_score : bool, default=False\u001b[39;00m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;124;03m        Whether to include train scores.\u001b[39;00m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;124;03m        Computing training scores is used to get insights on how different\u001b[39;00m\n\u001b[0;32m    244\u001b[0m \u001b[38;5;124;03m        parameter settings impact the overfitting/underfitting trade-off.\u001b[39;00m\n\u001b[0;32m    245\u001b[0m \u001b[38;5;124;03m        However computing the scores on the training set can be computationally\u001b[39;00m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;124;03m        expensive and is not strictly required to select the parameters that\u001b[39;00m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;124;03m        yield the best generalization performance.\u001b[39;00m\n\u001b[0;32m    248\u001b[0m \n\u001b[0;32m    249\u001b[0m \u001b[38;5;124;03m        .. versionadded:: 0.19\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \n\u001b[0;32m    251\u001b[0m \u001b[38;5;124;03m        .. versionchanged:: 0.21\u001b[39;00m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;124;03m            Default value was changed from ``True`` to ``False``\u001b[39;00m\n\u001b[0;32m    253\u001b[0m \n\u001b[0;32m    254\u001b[0m \u001b[38;5;124;03m    return_estimator : bool, default=False\u001b[39;00m\n\u001b[0;32m    255\u001b[0m \u001b[38;5;124;03m        Whether to return the estimators fitted on each split.\u001b[39;00m\n\u001b[0;32m    256\u001b[0m \n\u001b[0;32m    257\u001b[0m \u001b[38;5;124;03m        .. versionadded:: 0.20\u001b[39;00m\n\u001b[0;32m    258\u001b[0m \n\u001b[0;32m    259\u001b[0m \u001b[38;5;124;03m    return_indices : bool, default=False\u001b[39;00m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;124;03m        Whether to return the train-test indices selected for each split.\u001b[39;00m\n\u001b[0;32m    261\u001b[0m \n\u001b[0;32m    262\u001b[0m \u001b[38;5;124;03m        .. versionadded:: 1.3\u001b[39;00m\n\u001b[0;32m    263\u001b[0m \n\u001b[0;32m    264\u001b[0m \u001b[38;5;124;03m    error_score : 'raise' or numeric, default=np.nan\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;124;03m        Value to assign to the score if an error occurs in estimator fitting.\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[38;5;124;03m        If set to 'raise', the error is raised.\u001b[39;00m\n\u001b[0;32m    267\u001b[0m \u001b[38;5;124;03m        If a numeric value is given, FitFailedWarning is raised.\u001b[39;00m\n\u001b[0;32m    268\u001b[0m \n\u001b[0;32m    269\u001b[0m \u001b[38;5;124;03m        .. versionadded:: 0.20\u001b[39;00m\n\u001b[0;32m    270\u001b[0m \n\u001b[0;32m    271\u001b[0m \u001b[38;5;124;03m    Returns\u001b[39;00m\n\u001b[0;32m    272\u001b[0m \u001b[38;5;124;03m    -------\u001b[39;00m\n\u001b[0;32m    273\u001b[0m \u001b[38;5;124;03m    scores : dict of float arrays of shape (n_splits,)\u001b[39;00m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;124;03m        Array of scores of the estimator for each run of the cross validation.\u001b[39;00m\n\u001b[0;32m    275\u001b[0m \n\u001b[0;32m    276\u001b[0m \u001b[38;5;124;03m        A dict of arrays containing the score/time arrays for each scorer is\u001b[39;00m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;124;03m        returned. The possible keys for this ``dict`` are:\u001b[39;00m\n\u001b[0;32m    278\u001b[0m \n\u001b[0;32m    279\u001b[0m \u001b[38;5;124;03m            ``test_score``\u001b[39;00m\n\u001b[0;32m    280\u001b[0m \u001b[38;5;124;03m                The score array for test scores on each cv split.\u001b[39;00m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;124;03m                Suffix ``_score`` in ``test_score`` changes to a specific\u001b[39;00m\n\u001b[0;32m    282\u001b[0m \u001b[38;5;124;03m                metric like ``test_r2`` or ``test_auc`` if there are\u001b[39;00m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;124;03m                multiple scoring metrics in the scoring parameter.\u001b[39;00m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;124;03m            ``train_score``\u001b[39;00m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;124;03m                The score array for train scores on each cv split.\u001b[39;00m\n\u001b[0;32m    286\u001b[0m \u001b[38;5;124;03m                Suffix ``_score`` in ``train_score`` changes to a specific\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;124;03m                metric like ``train_r2`` or ``train_auc`` if there are\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;124;03m                multiple scoring metrics in the scoring parameter.\u001b[39;00m\n\u001b[0;32m    289\u001b[0m \u001b[38;5;124;03m                This is available only if ``return_train_score`` parameter\u001b[39;00m\n\u001b[0;32m    290\u001b[0m \u001b[38;5;124;03m                is ``True``.\u001b[39;00m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;124;03m            ``fit_time``\u001b[39;00m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;124;03m                The time for fitting the estimator on the train\u001b[39;00m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;124;03m                set for each cv split.\u001b[39;00m\n\u001b[0;32m    294\u001b[0m \u001b[38;5;124;03m            ``score_time``\u001b[39;00m\n\u001b[0;32m    295\u001b[0m \u001b[38;5;124;03m                The time for scoring the estimator on the test set for each\u001b[39;00m\n\u001b[0;32m    296\u001b[0m \u001b[38;5;124;03m                cv split. (Note time for scoring on the train set is not\u001b[39;00m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;124;03m                included even if ``return_train_score`` is set to ``True``\u001b[39;00m\n\u001b[0;32m    298\u001b[0m \u001b[38;5;124;03m            ``estimator``\u001b[39;00m\n\u001b[0;32m    299\u001b[0m \u001b[38;5;124;03m                The estimator objects for each cv split.\u001b[39;00m\n\u001b[0;32m    300\u001b[0m \u001b[38;5;124;03m                This is available only if ``return_estimator`` parameter\u001b[39;00m\n\u001b[0;32m    301\u001b[0m \u001b[38;5;124;03m                is set to ``True``.\u001b[39;00m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;124;03m            ``indices``\u001b[39;00m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;124;03m                The train/test positional indices for each cv split. A dictionary\u001b[39;00m\n\u001b[0;32m    304\u001b[0m \u001b[38;5;124;03m                is returned where the keys are either `\"train\"` or `\"test\"`\u001b[39;00m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;124;03m                and the associated values are a list of integer-dtyped NumPy\u001b[39;00m\n\u001b[0;32m    306\u001b[0m \u001b[38;5;124;03m                arrays with the indices. Available only if `return_indices=True`.\u001b[39;00m\n\u001b[0;32m    307\u001b[0m \n\u001b[0;32m    308\u001b[0m \u001b[38;5;124;03m    See Also\u001b[39;00m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;124;03m    --------\u001b[39;00m\n\u001b[0;32m    310\u001b[0m \u001b[38;5;124;03m    cross_val_score : Run cross-validation for single metric evaluation.\u001b[39;00m\n\u001b[0;32m    311\u001b[0m \n\u001b[0;32m    312\u001b[0m \u001b[38;5;124;03m    cross_val_predict : Get predictions from each split of cross-validation for\u001b[39;00m\n\u001b[0;32m    313\u001b[0m \u001b[38;5;124;03m        diagnostic purposes.\u001b[39;00m\n\u001b[0;32m    314\u001b[0m \n\u001b[0;32m    315\u001b[0m \u001b[38;5;124;03m    sklearn.metrics.make_scorer : Make a scorer from a performance metric or\u001b[39;00m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;124;03m        loss function.\u001b[39;00m\n\u001b[0;32m    317\u001b[0m \n\u001b[0;32m    318\u001b[0m \u001b[38;5;124;03m    Examples\u001b[39;00m\n\u001b[0;32m    319\u001b[0m \u001b[38;5;124;03m    --------\u001b[39;00m\n\u001b[0;32m    320\u001b[0m \u001b[38;5;124;03m    >>> from sklearn import datasets, linear_model\u001b[39;00m\n\u001b[0;32m    321\u001b[0m \u001b[38;5;124;03m    >>> from sklearn.model_selection import cross_validate\u001b[39;00m\n\u001b[0;32m    322\u001b[0m \u001b[38;5;124;03m    >>> from sklearn.metrics import make_scorer\u001b[39;00m\n\u001b[0;32m    323\u001b[0m \u001b[38;5;124;03m    >>> from sklearn.metrics import confusion_matrix\u001b[39;00m\n\u001b[0;32m    324\u001b[0m \u001b[38;5;124;03m    >>> from sklearn.svm import LinearSVC\u001b[39;00m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;124;03m    >>> diabetes = datasets.load_diabetes()\u001b[39;00m\n\u001b[0;32m    326\u001b[0m \u001b[38;5;124;03m    >>> X = diabetes.data[:150]\u001b[39;00m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;124;03m    >>> y = diabetes.target[:150]\u001b[39;00m\n\u001b[0;32m    328\u001b[0m \u001b[38;5;124;03m    >>> lasso = linear_model.Lasso()\u001b[39;00m\n\u001b[0;32m    329\u001b[0m \n\u001b[0;32m    330\u001b[0m \u001b[38;5;124;03m    Single metric evaluation using ``cross_validate``\u001b[39;00m\n\u001b[0;32m    331\u001b[0m \n\u001b[0;32m    332\u001b[0m \u001b[38;5;124;03m    >>> cv_results = cross_validate(lasso, X, y, cv=3)\u001b[39;00m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;124;03m    >>> sorted(cv_results.keys())\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;124;03m    ['fit_time', 'score_time', 'test_score']\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \u001b[38;5;124;03m    >>> cv_results['test_score']\u001b[39;00m\n\u001b[0;32m    336\u001b[0m \u001b[38;5;124;03m    array([0.3315057 , 0.08022103, 0.03531816])\u001b[39;00m\n\u001b[0;32m    337\u001b[0m \n\u001b[0;32m    338\u001b[0m \u001b[38;5;124;03m    Multiple metric evaluation using ``cross_validate``\u001b[39;00m\n\u001b[0;32m    339\u001b[0m \u001b[38;5;124;03m    (please refer the ``scoring`` parameter doc for more information)\u001b[39;00m\n\u001b[0;32m    340\u001b[0m \n\u001b[0;32m    341\u001b[0m \u001b[38;5;124;03m    >>> scores = cross_validate(lasso, X, y, cv=3,\u001b[39;00m\n\u001b[0;32m    342\u001b[0m \u001b[38;5;124;03m    ...                         scoring=('r2', 'neg_mean_squared_error'),\u001b[39;00m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;124;03m    ...                         return_train_score=True)\u001b[39;00m\n\u001b[0;32m    344\u001b[0m \u001b[38;5;124;03m    >>> print(scores['test_neg_mean_squared_error'])\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;124;03m    [-3635.5... -3573.3... -6114.7...]\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;124;03m    >>> print(scores['train_r2'])\u001b[39;00m\n\u001b[0;32m    347\u001b[0m \u001b[38;5;124;03m    [0.28009951 0.3908844  0.22784907]\u001b[39;00m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    349\u001b[0m     params \u001b[38;5;241m=\u001b[39m _check_params_groups_deprecation(fit_params, params, groups)\n\u001b[0;32m    351\u001b[0m     X, y \u001b[38;5;241m=\u001b[39m indexable(X, y)\n",
      "File \u001b[1;32mc:\\Users\\Hubert N\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:63\u001b[0m, in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, iterable):\n\u001b[0;32m     52\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Dispatch the tasks and return the results.\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \n\u001b[0;32m     54\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;124;03m    ----------\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;124;03m    iterable : iterable\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;124;03m        Iterable containing tuples of (delayed_function, args, kwargs) that should\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;124;03m        be consumed.\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \n\u001b[0;32m     60\u001b[0m \u001b[38;5;124;03m    Returns\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;124;03m    -------\u001b[39;00m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;124;03m    results : list\u001b[39;00m\n\u001b[1;32m---> 63\u001b[0m \u001b[38;5;124;03m        List of results of the tasks.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;66;03m# Capture the thread-local scikit-learn configuration at the time\u001b[39;00m\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;66;03m# Parallel.__call__ is issued since the tasks can be dispatched\u001b[39;00m\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;66;03m# in a different thread depending on the backend and on the value of\u001b[39;00m\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# pre_dispatch and n_jobs.\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     config \u001b[38;5;241m=\u001b[39m get_config()\n",
      "File \u001b[1;32mc:\\Users\\Hubert N\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1098\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1095\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1097\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1098\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretrieve()\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;66;03m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_time\n",
      "File \u001b[1;32mc:\\Users\\Hubert N\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:975\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    973\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    974\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupports_timeout\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m--> 975\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(job\u001b[38;5;241m.\u001b[39mget(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout))\n\u001b[0;32m    976\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    977\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(job\u001b[38;5;241m.\u001b[39mget())\n",
      "File \u001b[1;32mc:\\Users\\Hubert N\\anaconda3\\Lib\\site-packages\\joblib\\_parallel_backends.py:567\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    564\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[0;32m    565\u001b[0m \u001b[38;5;124;03mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[0;32m    566\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 567\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m future\u001b[38;5;241m.\u001b[39mresult(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    568\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CfTimeoutError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Hubert N\\anaconda3\\Lib\\concurrent\\futures\\_base.py:456\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m    457\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    458\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "File \u001b[1;32mc:\\Users\\Hubert N\\anaconda3\\Lib\\concurrent\\futures\\_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mBrokenProcessPool\u001b[0m: A task has failed to un-serialize. Please ensure that the arguments of the function are all picklable."
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "\n",
    "# Function to install a package\n",
    "def install_package(package_name):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package_name])\n",
    "\n",
    "# Install necessary libraries\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "except ModuleNotFoundError:\n",
    "    install_package(\"lightgbm\")\n",
    "    import lightgbm as lgb\n",
    "\n",
    "try:\n",
    "    from catboost import CatBoostClassifier\n",
    "except ModuleNotFoundError:\n",
    "    install_package(\"catboost\")\n",
    "    from catboost import CatBoostClassifier\n",
    "\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "except ModuleNotFoundError:\n",
    "    install_package(\"xgboost\")\n",
    "    import xgboost as xgb\n",
    "\n",
    "try:\n",
    "    from imblearn.over_sampling import SMOTE, ADASYN\n",
    "    from imblearn.combine import SMOTEENN\n",
    "except ModuleNotFoundError:\n",
    "    install_package(\"imbalanced-learn\")\n",
    "    from imblearn.over_sampling import SMOTE, ADASYN\n",
    "    from imblearn.combine import SMOTEENN\n",
    "\n",
    "try:\n",
    "    import optuna\n",
    "except ModuleNotFoundError:\n",
    "    install_package(\"optuna\")\n",
    "    import optuna\n",
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Machine Learning Libraries\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = r'C:\\Users\\Hubert N\\Downloads\\8.31.22 STAR Data Spreadsheet (2).csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Define columns of interest\n",
    "columns_of_interest = [\n",
    "    \"Sex (0 = female; 1 = male)\",\n",
    "    \"Neuter status (0 = castrated; 1 = not castrated)\",\n",
    "    \"Age\",\n",
    "    \"Duration of U-cath placement (days)\",\n",
    "    \"U-cath placement (no = 0; yes = 1)\",\n",
    "    \"Number of vet visits\",\n",
    "    'Collection Type (Cysto = 0; other =1; unknown = 2)',\n",
    "    'Color (0 = yellow/dark yellow/amber; 1 = straw; 2 = red/pink; 3 = brown/light & dark brown; 4 = other)',\n",
    "    'Clarity (0 = clear; 1 = opaque/cloudy; 2 = SLhazy; 3 = hazy)',\n",
    "    'USG', 'PH', 'Protein', 'Glucose', 'Ketones',\n",
    "    'Bilirubin', 'Hemoprotein', 'Sediment Vol', 'Volume',\n",
    "    'Lipid Layer', 'WBC (simplified)', 'Pyuria (1 if >/=4)',\n",
    "    'RBC (simplified)', 'Crystals (0 = none 1 = rare; 2 = few; 3 = mod; 4 = many)',\n",
    "    'Triple Phos', 'Bilirubin.1',\n",
    "    'Epithelial Cells-Transitional', 'Epithelial Cells-Squamous',\n",
    "    'Epithelial Cells-Renal', 'Epithelial Cells-Caudate',\n",
    "    'Casts-Hyaline', 'Casts-Granular',\n",
    "    'Bacteria quantity', 'Bac Type', 'Lipid Droplets', 'Sperm'\n",
    "]\n",
    "\n",
    "# Ensure all columns are present\n",
    "missing_columns = [col for col in columns_of_interest if col not in df.columns]\n",
    "if missing_columns:\n",
    "    print(\"Missing columns:\", missing_columns)\n",
    "    sys.exit()\n",
    "\n",
    "# Filter and encode the target variable\n",
    "df = df[df[\"Ampicillin\"].isin([\"R\", \"S\"])]  # Keep only 'R' and 'S' labels\n",
    "df['Ampicillin'] = df['Ampicillin'].map({'S': 0, 'R': 1})\n",
    "target = df['Ampicillin']\n",
    "\n",
    "# Select features\n",
    "features = df[columns_of_interest]\n",
    "\n",
    "# Manually define categorical columns\n",
    "categorical_cols = [\n",
    "    \"Sex (0 = female; 1 = male)\",\n",
    "    \"Neuter status (0 = castrated; 1 = not castrated)\",\n",
    "    \"U-cath placement (no = 0; yes = 1)\",\n",
    "    'Collection Type (Cysto = 0; other =1; unknown = 2)',\n",
    "    'Color (0 = yellow/dark yellow/amber; 1 = straw; 2 = red/pink; 3 = brown/light & dark brown; 4 = other)',\n",
    "    'Clarity (0 = clear; 1 = opaque/cloudy; 2 = SLhazy; 3 = hazy)',\n",
    "    'Lipid Layer', 'Pyuria (1 if >/=4)',\n",
    "    'Crystals (0 = none 1 = rare; 2 = few; 3 = mod; 4 = many)',\n",
    "    'Triple Phos', 'Bilirubin.1',\n",
    "    'Epithelial Cells-Transitional', 'Epithelial Cells-Squamous',\n",
    "    'Epithelial Cells-Renal', 'Epithelial Cells-Caudate',\n",
    "    'Casts-Hyaline', 'Casts-Granular',\n",
    "    'Bacteria quantity', 'Bac Type', 'Lipid Droplets', 'Sperm'\n",
    "]\n",
    "\n",
    "# Update categorical_cols based on available columns\n",
    "categorical_cols = [col for col in categorical_cols if col in features.columns]\n",
    "numerical_cols = [col for col in features.columns if col not in categorical_cols]\n",
    "\n",
    "# Data Preprocessing Enhancements\n",
    "\n",
    "# Impute missing values\n",
    "imputer_num = SimpleImputer(strategy='median')\n",
    "imputer_cat = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "features[numerical_cols] = imputer_num.fit_transform(features[numerical_cols])\n",
    "features[categorical_cols] = imputer_cat.fit_transform(features[categorical_cols])\n",
    "\n",
    "# Scale numerical features\n",
    "scaler = StandardScaler()\n",
    "features[numerical_cols] = scaler.fit_transform(features[numerical_cols])\n",
    "\n",
    "# One-hot encode categorical variables\n",
    "encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "encoded_cat = pd.DataFrame(encoder.fit_transform(features[categorical_cols]), index=features.index)\n",
    "encoded_cat.columns = encoder.get_feature_names_out(categorical_cols)\n",
    "\n",
    "# Combine scaled numerical and encoded categorical features\n",
    "features_preprocessed = pd.concat([features[numerical_cols], encoded_cat], axis=1)\n",
    "\n",
    "# Feature Engineering\n",
    "\n",
    "# Generate polynomial features\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "poly_features = poly.fit_transform(features_preprocessed)\n",
    "poly_feature_names = poly.get_feature_names_out(features_preprocessed.columns)\n",
    "features_poly = pd.DataFrame(poly_features, columns=poly_feature_names, index=features.index)\n",
    "\n",
    "# Feature Selection using RandomForest\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# Initial feature selection using RandomForest\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(features_poly, target)\n",
    "\n",
    "selector = SelectFromModel(rf, threshold='median', prefit=True)\n",
    "features_selected = selector.transform(features_poly)\n",
    "selected_feature_names = features_poly.columns[selector.get_support()]\n",
    "\n",
    "# Update features\n",
    "features_final = pd.DataFrame(features_selected, columns=selected_feature_names, index=features.index)\n",
    "\n",
    "# Handling Class Imbalance\n",
    "\n",
    "# Try different oversampling techniques and select the best one based on AUC score\n",
    "oversampling_methods = {\n",
    "    'SMOTE': SMOTE(random_state=42),\n",
    "    'ADASYN': ADASYN(random_state=42),\n",
    "    'SMOTEENN': SMOTEENN(random_state=42)\n",
    "}\n",
    "\n",
    "best_auc = 0\n",
    "best_method = None\n",
    "X_best_resampled = None\n",
    "y_best_resampled = None\n",
    "\n",
    "for method_name, sampler in oversampling_methods.items():\n",
    "    if method_name == 'SMOTEENN':\n",
    "        X_resampled, y_resampled = sampler.fit_resample(features_final, target)\n",
    "    else:\n",
    "        X_resampled, y_resampled = sampler.fit_resample(features_final, target)\n",
    "    \n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_resampled, y_resampled, test_size=0.2, random_state=42, stratify=y_resampled\n",
    "    )\n",
    "    \n",
    "    # Simple model for evaluation\n",
    "    model = RandomForestClassifier(random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    if auc > best_auc:\n",
    "        best_auc = auc\n",
    "        best_method = method_name\n",
    "        X_best_resampled = X_resampled\n",
    "        y_best_resampled = y_resampled\n",
    "\n",
    "# Use the best oversampling method\n",
    "print(f\"Best oversampling method: {best_method} with AUC: {best_auc:.4f}\")\n",
    "\n",
    "# Use the resampled data from the best method\n",
    "X_resampled = X_best_resampled\n",
    "y_resampled = y_best_resampled\n",
    "\n",
    "# Model Selection and Hyperparameter Tuning\n",
    "\n",
    "# Define models to try\n",
    "models = {\n",
    "    'LightGBM': lgb.LGBMClassifier(random_state=42),\n",
    "    'XGBoost': xgb.XGBClassifier(random_state=42, eval_metric='auc', use_label_encoder=False),\n",
    "    'RandomForest': RandomForestClassifier(random_state=42),\n",
    "    'CatBoost': CatBoostClassifier(random_state=42, eval_metric='AUC', silent=True),\n",
    "    'LogisticRegression': LogisticRegression(max_iter=1000, random_state=42)\n",
    "}\n",
    "\n",
    "# Split the data for testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_resampled, y_resampled, test_size=0.2, random_state=42, stratify=y_resampled\n",
    ")\n",
    "\n",
    "model_aucs = {}\n",
    "\n",
    "# Hyperparameter tuning for LightGBM using Optuna\n",
    "def objective(trial):\n",
    "    param = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'auc',\n",
    "        'verbosity': -1,\n",
    "        'boosting_type': 'gbdt',\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 150),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.2),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "        'subsample': trial.suggest_uniform('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1.0),\n",
    "        'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-5, 10),\n",
    "        'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-5, 10)\n",
    "    }\n",
    "    model = lgb.LGBMClassifier(**param, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    return auc\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "best_params = study.best_params\n",
    "best_lgb = lgb.LGBMClassifier(**best_params, random_state=42)\n",
    "best_lgb.fit(X_train, y_train)\n",
    "y_pred_proba = best_lgb.predict_proba(X_test)[:, 1]\n",
    "auc_lgb = roc_auc_score(y_test, y_pred_proba)\n",
    "model_aucs['LightGBM'] = auc_lgb\n",
    "print(f\"LightGBM AUC: {auc_lgb:.4f}\")\n",
    "\n",
    "# Evaluate other models\n",
    "for model_name, model in models.items():\n",
    "    if model_name == 'LightGBM':\n",
    "        continue  # Already evaluated\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    model_aucs[model_name] = auc\n",
    "    print(f\"{model_name} AUC: {auc:.4f}\")\n",
    "\n",
    "# Cross-Validation and Evaluation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_auc_scores = cross_val_score(best_lgb, X_resampled, y_resampled, cv=cv, scoring='roc_auc', n_jobs=-1)\n",
    "print(f\"Cross-Validated AUC Scores for LightGBM: {cv_auc_scores}\")\n",
    "print(f\"Mean Cross-Validated AUC Score for LightGBM: {cv_auc_scores.mean():.4f}\")\n",
    "\n",
    "# Ensemble Methods\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('lgb', best_lgb),\n",
    "        ('xgb', models['XGBoost']),\n",
    "        ('rf', models['RandomForest']),\n",
    "        ('cat', models['CatBoost'])\n",
    "    ],\n",
    "    voting='soft',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "voting_clf.fit(X_train, y_train)\n",
    "y_pred_proba_voting = voting_clf.predict_proba(X_test)[:, 1]\n",
    "auc_voting = roc_auc_score(y_test, y_pred_proba_voting)\n",
    "model_aucs['VotingClassifier'] = auc_voting\n",
    "print(f\"VotingClassifier AUC: {auc_voting:.4f}\")\n",
    "\n",
    "# Print the highest AUC score achieved\n",
    "highest_auc_model = max(model_aucs, key=model_aucs.get)\n",
    "highest_auc = model_aucs[highest_auc_model]\n",
    "print(f\"\\nHighest AUC Score Achieved: {highest_auc:.4f} with {highest_auc_model}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
